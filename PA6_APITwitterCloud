package edu.brandeis.cs12b.PA6_Solution;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;

import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.en.EnglishAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.json.JSONException;
import org.json.JSONObject;

import com.twitter.hbc.ClientBuilder;
import com.twitter.hbc.core.Client;
import com.twitter.hbc.core.Constants;
import com.twitter.hbc.core.Hosts;
import com.twitter.hbc.core.HttpHosts;
import com.twitter.hbc.core.endpoint.StatusesFilterEndpoint;
import com.twitter.hbc.core.processor.StringDelimitedProcessor;
import com.twitter.hbc.httpclient.auth.Authentication;
import com.twitter.hbc.httpclient.auth.OAuth1;

import wordcloud.CollisionMode;
import wordcloud.WordCloud;
import wordcloud.WordFrequency;
import wordcloud.bg.RectangleBackground;
import wordcloud.font.scale.LinearFontScalar;

public class TwitterCloud {
	
	/**
	 * The number of tokens you should extract from tweets
	 */
	private static final int NUMBER_TOKENS = 4000;
	Map <String, Integer> map= new HashMap <String, Integer> ();
	List<WordFrequency> list= new ArrayList <> ();
	
	List<String> tokens = new LinkedList<String>(); 
	private String[] args;
		
	public static void main(String[] a) throws InterruptedException{
		TwitterCloud tc = new TwitterCloud();
		tc.makeCloud(new String[] { "donald", "trump" }, "test.png");
	}


	
	public void makeCloud(String[] args, String filename) throws InterruptedException{
		this.args = args;
		
		Hosts hosebirdHosts = new HttpHosts(Constants.STREAM_HOST); 
		StatusesFilterEndpoint hosebirdEndpoint = new StatusesFilterEndpoint();
		
		List<String> terms = Arrays.asList(args);
		hosebirdEndpoint.trackTerms(terms);
		Authentication hosebirdAuth = new OAuth1("36M2WU4olevEOqYSKh0N2KTi9", System.getenv("CONSUMER_SECRET"),  "1730765472-rfxjdivxxcla00GC45L2KEIX5oB1130rJsZwnFi", System.getenv("TOKEN_SECRET"));
		BlockingQueue<String> msgQueue = new LinkedBlockingQueue<String>(100000);
		
		ClientBuilder builder = new ClientBuilder()
				
				.hosts(hosebirdHosts) 
				.authentication(hosebirdAuth) 
				.endpoint(hosebirdEndpoint) 
				.processor(new StringDelimitedProcessor(msgQueue));
				Client hosebirdClient = builder.build(); 
				hosebirdClient.connect();
	
	
		while (tokens.size() < NUMBER_TOKENS) { 
			String msg = msgQueue.take();
			onPostExecute(msg);
		}
		converter();
		build();
	
	}
	private void onPostExecute (String result){
		
		try {
			JSONObject resultObject= new JSONObject (result);
			String line= resultObject.getString("text");
			// EnglishAnalyzer class is implemented to make program loop until it has collected the correct number of unique stemmed tokens
			try (EnglishAnalyzer an = new EnglishAnalyzer()) { 
				TokenStream sf = an.tokenStream(null, line);
				try { 
					sf.reset(); 
					while (sf.incrementToken()) { 
						// method that converts word to stem value
						CharTermAttribute cta = sf.getAttribute(CharTermAttribute.class); 
						tokens.add(cta.toString());
						if (map.containsKey(cta.toString())){
							int countCurrent= map.get(cta.toString());
							map.put(cta.toString(), countCurrent + 1);		
						}
						else{
							map.put(cta.toString(), 1);
						}
					} 
					} catch (Exception e) { 
						System.err.println("Could not tokenize string: " + e); 
					}
			} 	
		} catch (JSONException e){
			
		}
	}
	public void converter (){
		for (String s: map.keySet()){
			int store= map.get(s);
			WordFrequency name= new WordFrequency (s, store);
			list.add(name);
		}
	}
	public void build(){
		WordCloud wordCloud = new WordCloud(400, 400, CollisionMode.RECTANGLE); 
		wordCloud.setPadding(0); 
		wordCloud.setBackground(new RectangleBackground(400, 400)); 
		wordCloud.setFontScalar(new LinearFontScalar(14, 40)); 
		wordCloud.build(list); 
		wordCloud.writeToFile("test.png");
	}
}
